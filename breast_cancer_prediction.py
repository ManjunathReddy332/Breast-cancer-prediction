# -*- coding: utf-8 -*-
"""Breast_cancer_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yQ94qofRznOys9DFbGSpzvfJ0PbXTkIl

**Breast cancer prediction using Neural Networks**

Importing Libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sklearn.datasets
from sklearn.model_selection import train_test_split

"""Load the dataset

In this dataset, malignant means having high chances of suffering from cancer, whereas belignant does not mean cancer
"""

data=sklearn.datasets.load_breast_cancer()

print(data)

"""Data preprocessing"""

#converting dictionary into pandas dataframe. here feature_names are the column names
df=pd.DataFrame(data.data, columns=data.feature_names)

df.head()

df['label']=data.target

df.tail()

df.shape

df.info()

df.isnull().sum()

df.describe()

df['label'].value_counts()

df.groupby('label').mean()

"""Seperating features and target"""

x=df.drop(columns='label', axis=1)     #considering all the columns except label column as features
y=df['label']

"""Splitting the dataset into train and test data"""

x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=2)  #splitting 20% and 80% of the data into test and train data

print(x.shape, x_train.shape, x_test.shape)

"""Standardizing the data"""

from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()                                 #we will standardize the data to get high accuracy and less loss
x_train_std=scaler.fit_transform(x_train)
x_test_std=scaler.transform(x_test)

"""Building Neural Networks"""

import tensorflow as tf                 #used to create the neural networks
tf.random.set_seed(3)
from tensorflow import keras

model = keras.Sequential([
                          keras.layers.Flatten(input_shape=(30,)),    #input layer
                          keras.layers.Dense(20, activation='relu'),   #hidden layers
                          keras.layers.Dense(2, activation='sigmoid')   #output layer
])

#compiling the NN
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

#traning the model
history = model.fit(x_train_std, y_train, validation_split=0.1, epochs=10)

"""Visualizing accuracy and loss"""

#epochs vs accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])

plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')

plt.legend(['training data', 'validation data'], loc = 'lower right')

#epochs vs loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])

plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')

plt.legend(['training data', 'validation data'], loc = 'upper right')

"""Accuracy of the model on test data"""

#checking loss and accuracy of test data
loss,accuracy=model.evaluate(x_test_std, y_test)
print(accuracy)

#considering first row of test data
print(x_test_std.shape)
print(x_test_std[0])

y_pred=model.predict(x_test_std)

print(y_pred.shape)
print(y_pred[0])

print(y_pred)

#argmax gives the index of the larger value
list=[0.52, 0.65]
print(np.argmax(list))

y_pred_labels=[np.argmax(i) for i in y_pred]
print(y_pred_labels)

"""Predictive system"""

input=(13.07,11.76,66.72,274.9,0.07684,0.05513,0.06734,0.01211,0.1432,0.05787,0.4062,1.18,2.635,28.47,0.006534,0.008796,0.07835,0.007445,0.02406,0.001769,12.98,25.72,82.98,516.5,0.1132,0.09721,0.04777,0.03517,0.2543,0.06423)

#change the input into a numpy array
arr=np.asarray(input)

#reshaping the numpy array
arr2=arr.reshape(1,-1)

#standardizing the input data
input_std=scaler.transform(arr2)

prediction=model.predict(input_std)
print(prediction)

prediction_label=[np.argmax(prediction)]
print(prediction_label)

if(prediction_label[0]==0):
  print('The tumor is Malignant')

else:
  print('The tumor is Benign')